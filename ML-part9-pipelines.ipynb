{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate Machine Learning Workflows with Pipelines\n",
    "\n",
    "1. How to use pipelines to minimize <span class=\"burk\"><span class=\"burk\"><span class=\"girk\">data leakage</span></span></span>.\n",
    "2. How to construct a data preparation and modeling pipeline.\n",
    "3. How to construct a feature extraction and modeling pipeline.\n",
    "\n",
    "There are <span class=\"mark\">standard workflows</span> in applied machine learning. <span class=\"mark\">Standard because they overcome</span> <span class=\"mark\">common problems like data leakage</span> in your test harness. Python scikit-learn provides a Pipeline utility to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can\n",
    "be evaluated. <span class=\"mark\">The goal</span> is to ensure that <span class=\"girk\">all of the steps in the pipeline are constrained to the data available for the evaluation</span>, such as the training dataset or each fold of the cross validation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Modeling Pipeline\n",
    "An <span class=\"burk\">easy trap to fall</span> into in applied machine learning is <span class=\"burk\">leaking data from your training dataset to your test dataset</span>. To <span class=\"girk\">avoid this trap</span> you need a robust test harness with <span class=\"girk\">strong separation of training and testing</span>. This includes data preparation. Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm. For example, preparing your data\n",
    "using <span class=\"mark\">normalization or standardization on the entire training dataset before learning</span> would <span class=\"burk\">not be a valid test</span> because the training dataset would have been <span class=\"burk\">influenced by the scale of the data in the test set</span>.\n",
    "\n",
    "<span class=\"girk\">Pipelines help you prevent data leakage</span> in your test harness by ensuring that <span class=\"mark\">data preparation like standardization is constrained to each fold</span> of your cross validation procedure. The example below demonstrates this important data preparation and model evaluation workflow on the Pima Indians onset of diabetes dataset. The pipeline is defined with two steps:\n",
    "\n",
    "1. Standardize the data.\n",
    "2. Learn a Linear Discriminant Analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pima Indians Diabetes Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#Loading dataset\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv('pima-indians-diabetes.data',names=names)\n",
    "\n",
    "# separate array into input and output components\n",
    "X = df.drop('class',axis='columns')\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064252\n"
     ]
    }
   ],
   "source": [
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize',StandardScaler()))\n",
    "estimators.append(('lda',LinearDiscriminantAnalysis()))\n",
    "\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Modeling Pipeline\n",
    "<span class=\"mark\">Feature extraction</span> is another procedure that <span class=\"burk\">is susceptible to data leakage</span>. Like data preparation, feature extraction procedures <span class=\"mark\">must be restricted to the data in your training dataset</span>. The pipeline provides a handy tool called the FeatureUnion which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a\n",
    "model can be trained. Importantly, <span class=\"girk\">all the feature extraction and the feature union occurs within each fold of the cross validation procedure</span>. The example below demonstrates the pipeline\n",
    "defined with four steps:\n",
    "\n",
    "1. Feature Extraction with Principal Component Analysis (3 features).\n",
    "2. Feature Extraction with Statistical Selection (6 features).\n",
    "3. Feature Union.\n",
    "4. Learn a Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca',PCA(n_components=3)))\n",
    "features.append(('select_best',SelectKBest(k=6)))\n",
    "\n",
    "feature_union = FeatureUnion(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776042378674\n"
     ]
    }
   ],
   "source": [
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union',feature_union))\n",
    "estimators.append(('logistic',LogisticRegression()))\n",
    "\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
