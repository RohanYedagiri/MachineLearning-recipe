{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Performance with Ensembles\n",
    "<span class=\"mark\">Ensembles</span> can give you a <span class=\"girk\">boost in accuracy</span> on your dataset. In this chapter you will discover how you can create some of the most powerful types of ensembles in Python using scikit-learn. This lesson will step you through Boosting, Bagging and Majority Voting and show you how you can continue to ratchet up the accuracy of the models on your own datasets. After completing\n",
    "this lesson you will know:\n",
    "\n",
    "1. How to use <span class=\"mark\">bagging</span> ensemble methods such as <span class=\"girk\">bagged decision trees, random forest and extra trees</span>.\n",
    "2. How to use <span class=\"mark\">boosting</span> ensemble methods such as <span class=\"girk\">AdaBoost and stochastic gradient boosting</span>.\n",
    "3. How to use <span class=\"mark\">voting ensemble</span> methods to <span class=\"mark\">combine</span> the predictions from <span class=\"girk\">multiple algorithms</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Models Into Ensemble Predictions\n",
    "\n",
    "- <span class=\"mark\">Bagging</span>: Building <span class=\"mark\"><span class=\"mark\">multiple models</span></span> (typically of the <span class=\"girk\">same type</span>) from <span class=\"mark\">different subsamples of the training dataset</span>.\n",
    "- <span class=\"mark\">Boosting</span>: Building <span class=\"burk\">multiple models (typically of the same type</span>) each of which <span class=\"girk\">learns to fix the prediction errors of a prior model</span> in the sequence of models.\n",
    "- <span class=\"mark\">Voting</span>: Building multiple models (typically of <span class=\"mark\">differing types</span>) and simple statistics (like calculating the mean) are used to <span class=\"girk\">combine predictions</span>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Decision Trees\n",
    "<span class=\"mark\">Bagging performs best with algorithms that have high variance</span>. A popular <span class=\"burk\">example are decision trees</span>, often constructed without pruning. In the example below is an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier1). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pima Indians Diabetes Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Loading dataset\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv('pima-indians-diabetes.data',names=names)\n",
    "\n",
    "# separate array into input and output components\n",
    "X = df.drop('class',axis='columns')\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770745044429\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=100, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random Forests is an extension of bagged decision trees.\n",
    "\n",
    "<span class=\"mark\">Samples of the training dataset are taken with replacement,</span> but the <span class=\"mark\">trees are constructed</span> in a way that <span class=\"girk\">reduces the correlation\n",
    "between individual classifiers</span>.\n",
    "\n",
    "<span class=\"burk\">Specifically, rather than greedily</span> choosing the best split point in the construction of each tree, only a <span class=\"girk\">random subset of features are considered for each split</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757775119617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_features=3)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees\n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762952836637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=100, max_features=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that <span class=\"girk\">attempt to correct the mistakes of the models before them</span> in the sequence. Once created, the <span class=\"mark\">models make predictions</span> which may be <span class=\"girk\">weighted by their demonstrated accuracy</span> and the <span class=\"mark\">results are combined to create a final output prediction</span>. The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "- AdaBoost.\n",
    "- Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by <span class=\"mark\">weighting instances in the dataset</span> by how easy or difficult they are to classify, allowing the algorithm to pay more or less attention to them in the construction of subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76045796309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766900205058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first <span class=\"mark\">creating two or more standalone models from your training</span> dataset. A <span class=\"mark\">Voting Classifier</span> can then be <span class=\"girk\">used to wrap your models and average the predictions</span> of the sub-models when asked to make predictions for new data. The predictions of the sub-models can\n",
    "be weighted, but <span class=\"mark\">specifying the weights for classifiers manually or even heuristically is difficult.</span> More advanced methods can learn how to best weight the predictions from sub-models, but this is called <span class=\"burk\">stacking (stacked aggregation)</span> and is currently not provided in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734278879016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = []\n",
    "\n",
    "estimators.append(('log',LogisticRegression()))\n",
    "estimators.append(('cart',DecisionTreeClassifier()))\n",
    "estimators.append(('svm',SVC()))\n",
    "\n",
    "\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
