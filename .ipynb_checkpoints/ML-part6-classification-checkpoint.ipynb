{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spot-Check Classification Algorithms\n",
    "You cannot know which algorithms are best suited to your problem beforehand. You\n",
    "must trial a number of methods and focus attention on those that prove themselves the most\n",
    "promising.\n",
    "\n",
    "The question is not: <span class=\"girk\">What algorithm should I use on my dataset? Instead it is: What\n",
    "algorithms should I spot-check on my dataset?</span>\n",
    "\n",
    "### I recommend trying a mixture of algorithms and see what is good at picking out the structure in your data. Below are some suggestions when spot-checking algorithms on your dataset:\n",
    "\n",
    "- Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "- Try a mixture of learning algorithms (e.g. different algorithms for learning the same type of representation).\n",
    "- Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Overview\n",
    "We are going to take a look at six classification algorithms that you can spot-check on your\n",
    "dataset. Starting with two linear machine learning algorithms:\n",
    "\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "Then looking at four nonlinear machine learning algorithms:\n",
    "- k-Nearest Neighbors.\n",
    "- Naive Bayes.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use two linear machine learning algorithms:\n",
    "logistic regression and linear discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression <span class=\"mark\">assumes a Gaussian distribution for the numeric input variables</span> and can\n",
    "model binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pima Indians Diabetes Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = pd.read_csv('pima-indians-diabetes.data',names=names)\n",
    "\n",
    "# separate array into input and output components\n",
    "X = df.drop('class',axis='columns')\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76951469583\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "final = []\n",
    "final.append(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis or LDA is a statistical technique <span class=\"mark\">for binary and multiclass\n",
    "classification</span>. It too assumes a <span class=\"mark\">Gaussian distribution for the numerical input variables.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064252\n"
     ]
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "final.append(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Machine Learning Algorithms\n",
    "### k-Nearest Neighbors\n",
    "The k-Nearest Neighbors algorithm (or KNN) <span class=\"mark\">uses a distance metric</span> to \f",
    "nd the k most similar\n",
    "instances in the training data for a new instance and takes the <span class=\"mark\">mean outcome of the neighbors\n",
    "as the prediction</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726555023923\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "final.append(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "- Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value.\n",
    "- These probabilities are estimated for new data and multiplied together, <span class=\"mark\">assuming that they are all independent</span> (a simple or naive assumption).\n",
    "- When working with <span class=\"mark\">real-valued data</span>, a <span class=\"mark\">Gaussian distribution</span> is assumed <span class=\"girk\">to easily estimate the probabilities</span> for input variables <span class=\"mark\">using the Gaussian Probability Density Function.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75517771702\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "final.append(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression Trees\n",
    "Classification and Regression Trees (CART or just decision trees) <span class=\"mark\">construct a binary tree</span> from\n",
    "the training data. <span class=\"girk\">Split points are chosen greedily</span> by evaluating each attribute and each value\n",
    "of each attribute in the training data <span class=\"girk\">in order to minimize a cost function</span> (like the Gini index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692634996582\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "final.append(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "Support Vector Machines (or SVM) seek <span class=\"mark\">a line that best separates two classes</span>. Those data instances that are <span class=\"mark\">closest to the line</span> that best separates the classes <span class=\"girk\">are called support vectors</span> and influence where the line is placed. <span class=\"girk\">SVM</span> has been <span class=\"mark\">extended to support multiple classes</span>. Of <span class=\"girk\">particular importance</span> is the <span class=\"mark\">use of different kernel functions</span> via the kernel parameter. A powerful <span class=\"girk\">Radial Basis Function is used by default</span>. You can construct an SVM model using the SVC class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651025290499\n"
     ]
    }
   ],
   "source": [
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "final.append(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.76951469583048526,\n",
       " 0.77346206425153796,\n",
       " 0.72655502392344506,\n",
       " 0.75517771701982228,\n",
       " 0.69263499658236494,\n",
       " 0.65102529049897473]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelsList = ['Logistic regression','lda','knn','naive','decisiontree','svm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lda</th>\n",
       "      <td>0.773462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.769515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive</th>\n",
       "      <td>0.755178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.726555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decisiontree</th>\n",
       "      <td>0.692635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.651025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "lda                  0.773462\n",
       "Logistic regression  0.769515\n",
       "naive                0.755178\n",
       "knn                  0.726555\n",
       "decisiontree         0.692635\n",
       "svm                  0.651025"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdict = pd.DataFrame.from_dict(dict(zip(modelsList,final)),orient='index')\n",
    "resdict.sort_values(0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear algos did great, but only for this dataset, what happens if there are categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
